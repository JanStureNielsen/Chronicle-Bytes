= Chronicle Bytes Architecture Overview
:doctype: book
:toc:
:toclevels: 3
:sectnums:
:lang: en-GB

This document positions Chronicle Bytes within the wider OpenHFT landscape, describes its core abstractions and key architectural principles, illustrates runtime interactions, and explains the critical data flows and design decisions that underpin its low-latency performance.
It is intended for developers and architects working with or building upon the OpenHFT stack.

== 1  Context and Core Abstractions

Chronicle Bytes is the **foundational byte-access layer** for every OpenHFT library that deals with off-heap memory, serialisation or durable messaging.
It hides the platform-specific details of raw memory manipulation behind a small, predictable API while exposing advanced features (elastic buffers, atomic CAS, stop-bit encoding) that higher layers consume.

The primary abstractions are:

* `BytesStore<B, U>`: Represents the underlying, typically immutable, block of memory.
This can be on-heap (`byte[]`), native off-heap (direct memory), or memory-mapped file regions <<CB-FN-002>>.
* `Bytes<U>`: Provides a mutable view over a `BytesStore`, with independent read and write cursors (positions) and limits.
It offers a rich API for reading and writing primitive types, strings, and other data structures
<<CB-FN-001>>.
* `RandomDataInput` / `RandomDataOutput`: Interfaces implemented by `Bytes`
(among others) offering position-based read/write access, distinct from the streaming cursor-based access.

[NOTE]
====
Where a statement maps directly to a formal requirement from
`project-requirements.adoc`, the identifier is shown in brackets, e.g.
<<CB-FN-004>>.
====

=== 1.1  Component Relationships

Chronicle Bytes maintains a clear separation between the region of memory and the
view used to access it. `BytesStore` represents the memory itself while `Bytes`
is an accessor with movable cursors. Several concrete implementations exist for
different storage modes:

....
            Bytes
              |
   +----------+----------+
   |          |          |
NativeBytes MappedBytes OnHeapBytes
   |          |          |
   |          |          |
NativeBytesStore MappedBytesStore HeapBytesStore
....

Each `Bytes` variant wraps a specific `BytesStore`. A `BytesStore` may be shared
between many `Bytes` views, but each `Bytes` instance has its own read and write
cursors.

=== 1.2  Memory Types

* **On-heap**: Backed by a `byte[]`. GC manages the memory but introduces
  unpredictable pauses.
* **Native (off-heap)**: Backed by direct memory outside the Java heap. It avoids
  GC pauses and is suitable for low-latency paths.
* **Memory-mapped**: Regions of files mapped into memory via `MappedBytesStore`.
  Useful for IPC and persistence with minimal copying.

=== 1.3  Threading Model (<<CB-NF-O-005>>)

`Bytes` instances are not thread-safe for concurrent mutation. Each thread should
use its own `Bytes` view. Atomic operations on a `BytesStore` (`compareAndSwapLong`
and similar) are safe to call from multiple threads.

=== 1.4  Elastic Buffers

Elastic `Bytes` instances, such as those created with
`Bytes.allocateElasticDirect()`, grow on demand up to the capacity of their
underlying `BytesStore`. Growth involves allocating a larger store and copying
existing data, so pre-sizing buffers can avoid a stall on the first resize.

=== 1.5  Reference Counting

Off-heap `BytesStore` implementations use reference counting to manage native
memory. Calls to `reserve()` increment the count, while `release()` and
`releaseLast()` decrement it. When the count reaches zero the memory is freed or
the mapping is unmapped. Forgetting to release results in leaks; releasing too
many times throws `IllegalStateException`.

== 2  Guiding Architectural Principles

The design and implementation of Chronicle Bytes adhere to several core principles to meet the demands of high-performance systems:

* **Low Latency by Design:** Every feature and API is scrutinised for its latency impact, aiming for sub-microsecond access times for common operations.
* **Minimal Garbage Collection (GC) Impact:** Prioritising off-heap storage (direct memory and memory-mapped files) for critical data paths to avoid GC pauses.
* **Mechanical Sympathy:** Structuring data and access patterns to align with how underlying hardware (CPUs, memory hierarchy, OS) operates efficiently.
* **Zero-Copy Operations:** Enabling data manipulation and transfer without redundant intermediate copies where feasible.
* **Deterministic Resource Management:** Providing explicit control over off-heap memory lifecycle via reference counting and `Closeable` interfaces
<<CB-NF-O-001>>.

== 3  Container Diagram (C4 level 2)

The system can be visualized at a container level as follows, showing components within a typical High-Frequency Trading (HFT) application:

....
+------------------------------------------------------------+
| JVM A  (Market-Data Feed-Handler)                          |
|                                                            |
|  +-------------------+  +------------------------------+   |
|  | Chronicle Queue   |  | Business Logic / Deserialise |   |
|  | (uses MappedBytes)|  | via Chronicle Wire           |   |
|  |  ^  |              |  +------------------------------+   |
|  +--|--+--------------+                                   |
|     |  writes raw Bytes (CB-FN-001)                        |
+-----|------------------------------------------------------+
      |    memory-mapped file (off-heap, CB-FN-002)          |
......|......................................................
      v
+------------------------------------------------------------+
| JVM B  (Risk Engine / Matching Engine)                       |
|                                                            |
|  +------------------------------+  consumes Bytes via      |
|  | Chronicle Queue Tailer       |<-------------------------+
|  | (uses MappedBytes)           |                          |
|  +------------------------------+                          |
|  | Risk Check & Matching Logic  |                          |
|  +------------------------------+                          |
|  | Chronicle Map (Positions)    |  atomic updates using    |
|  | (uses OffHeapBytes)          |  Bytes' CAS (CB-FN-004)  |
|  +------------------------------+                          |
|                                                            |
+------------------------------------------------------------+

....

Key points:

* **Single-writer, multi-reader** pattern on the Chronicle Queue, leveraging
`MappedBytes` for shared memory-mapped files, removes the need for locks during message exchange.
* Both containers (JVMs) interact with the same memory-mapped files; IPC latency is often the cost of an L3 cache miss plus pointer arithmetic.
* Risk logic updates positions in an off-heap Chronicle Map; contention is mitigated via `Bytes`' Compare-And-Swap (CAS) operations.

== 4  Data-Flow Narrative

=== 4.1 Hot Path (Low-Latency Event Processing)

. **Feed handler** (JVM A) receives binary wire data from the network.
It parses this and appends each message into a `MappedBytes` segment (a
`Bytes<T>` view over a `MappedBytesStore`).
Chronicle Wire may be used here for efficient serialisation directly into the `Bytes` buffer.
The write path adds minimal GC pressure as the memory is already mapped and managed off-heap
<<CB-NF-P-003>>.
. **Queue tailer** in JVM B polls the same memory-mapped file pages.
As new data is written by JVM A (and memory barriers are handled by Chronicle Queue internals), it becomes visible to JVM B. Chronicle Wire deserialises the byte stream from its `Bytes` view into Java objects or direct method invocations, depending on configuration.
. **Risk engine** in JVM B performs atomic adjustments on an entry within an off-heap `Chronicle Map`.
The update (e.g., incrementing a position or changing a status) might be a single 64-bit CAS operation directly on the
`BytesStore` backing the map, producing no garbage (`compareAndSwapLong`,
<<CB-FN-004>>).
. If an order passes risk checks, a response message may be written to a
*second* Chronicle Queue (again, using `MappedBytes`) for further processing or sending to an exchange gateway.

Typical latency budget (one-way, in-host, for Chronicle Bytes specific parts):

* **Append to queue (Bytes level):** Sub-microsecond for writing raw bytes (actual serialisation via Wire adds to this, e.g., YAML as in <<CB-NF-P-001>>).
* **Tailer read (Bytes level):** Sub-microsecond for reading raw bytes (deserialisation via Wire adds to this).
* **CAS on position map (Bytes level):** ~70 ns mean (hot cache for the
`BytesStore` location) â€“ meets <<CB-NF-P-004>>.

=== 4.2 Cold Path Example (Resource Initialization)

During application startup or when a new trading day begins (a "cold path" scenario), Chronicle Queue might create a new multi-gigabyte memory-mapped file.
This involves Chronicle Bytes allocating and configuring a `MappedBytesStore` for the new file.
This process includes interactions with the operating system to map the file into the process's address space.
Chronicle Bytes may also pre-touch memory pages within the `MappedBytesStore` to ensure more predictable access times once the hot path operations begin.
Similarly, Chronicle Map, upon startup, might load its initial dataset from a persisted file into an off-heap `BytesStore`, involving potentially large block reads and writes managed by `Bytes` instances.
These operations are less frequent and less latency-sensitive than hot path processing but are critical for system setup and resilience.

== 5  Chronicle Bytes in the OpenHFT Stack

Chronicle Bytes serves as the fundamental building block for data handling in several key OpenHFT libraries:

* **Chronicle Wire:** Uses `Bytes` instances (on-heap, off-heap, or mapped) as the underlying buffer for its various serialization protocols (binary, text, YAML, JSON, raw).
This enables efficient, low-GC (de)serialization directly to/from any memory supported by Bytes.
* **Chronicle Queue:** Relies heavily on `MappedBytes` (a `Bytes` implementation backed by `MappedBytesStore`) for its persistent message storage.
This facilitates extremely fast inter-process communication (IPC) via shared memory-mapped files.
* **Chronicle Map:** Utilizes off-heap `BytesStore` implementations to store its keys and values, allowing for very large, GC-free, persistent (if backed by MMFs) key-value stores.
It leverages atomic operations from `Bytes` (e.g., `compareAndSwapLong`) for thread-safe concurrent access.
* **Chronicle FIX (and other protocol engines):** Often use `Bytes` for parsing and constructing messages directly from/to network buffers or memory-mapped files, minimizing copying and GC overhead.

== 6  Key Design Decisions

[cols="1,5"]
|===
|Decision |Rationale
|Off-heap first |Avoids GC pauses that would violate micro-second Service Level Agreements (SLAs), aligning with minimal GC impact principle.
|Memory-mapped durability via `BytesStore` |Provides *in-memory speed* for access and *on-disk safety* for durability using a unified `Bytes` API.
|Native endianness as default |Removes conditional branches in hot paths for performance; readers from different endian systems can detect mismatch and fail fast (CB-RISK-001).
|Elastic buffers by default (`Bytes.elasticXXX()`) |Simplifies API usage for common cases while preserving performance and predictability; growth is bounded by `BytesStore.realCapacity()` (CB-FN-003).
|Atomic field operations within `Bytes` API |Reduces boilerplate and potential errors in user code for lock-free algorithms, avoiding `java.util.concurrent.atomic` object overheads for off-heap memory (CB-FN-004).
|Reference counting for off-heap resources |Enables deterministic reclamation of native memory, crucial for predictable system behaviour, unlike reliance on GC and finalizers alone (CB-NF-O-001).
|===

== 7  Trade-Offs and Alternatives Considered

* **Netty `ByteBuf` or Agrona `DirectBuffer`:** These libraries provide similar low-level buffer primitives.
However, neither integrates as seamlessly out-of-the-box with the Chronicle ecosystem (Wire, Queue, Map), which are designed around the `Bytes` and `BytesStore` abstractions.
Adopting them would require introducing adapter layers, potentially adding overhead or complexity.
* **Heap buffers (`byte[]` wrapped by `java.nio.ByteBuffer`):** While simpler from a memory management perspective (GC handles it), they reintroduce GC jitter on critical paths, which is unacceptable for most OpenHFT use cases.
Chronicle Bytes *supports* on-heap buffers for flexibility but emphasizes off-heap for performance-sensitive paths.
* **Custom memory mapping + `sun.misc.Unsafe`:** Writing custom memory mapping and direct memory access logic using only `Unsafe` would offer raw speed.
However, this approach would duplicate much of the safety, utility (e.g., elasticity, typed accessors, bounds checking), and resource management code already robustly implemented and tested in Chronicle Bytes.
The maintenance burden and risk of errors were considered too high.

== 8  Alignment with Requirements

This architecture directly supports and enables the fulfilment of the functional and performance requirements captured in `project-requirements.adoc`:

* Core API exposure for various memory types and operations (CB-FN-001 .. CB-FN-008, CB-FN-012, CB-FN-014).
* Performance targets for serialization, access speed, and atomic operations (e.g., <<CB-NF-P-001>>, <<CB-NF-P-003>>, <<CB-NF-P-004>>).
* Operability hooks like JMX metrics for `BytesMetrics` (CB-NF-O-002) allow integration into standard monitoring dashboards.
* Security considerations like robust bounds checking (CB-NF-S-001) are integral to the `Bytes` API design.

== 9  Future Evolution

* **Adoption of JDK advancements:** Continue to leverage new JVM intrinsics and APIs like Project Panama/Foreign Function & Memory API (when production-ready and performant) or Vector API (e.g., `VectorizedMismatch` in Java 21+) to accelerate operations like `Bytes.equalTo()` or bulk data manipulation, aiming to exceed targets like <<CB-NF-P-002>>.
* **Enhanced OS integration:** Explore deeper OS-level features like
`userfaultfd` on Linux for more efficient page fault handling in memory-mapped files, potentially improving fsync performance without kernel round-trips.
* **NUMA-aware allocation:** Investigate strategies for Non-Uniform Memory Access (NUMA) aware segment allocation for `BytesStore` instances, particularly for `MappedBytesStore`, to reduce cross-socket memory access latency on multi-socket server architectures.
